{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Basic Logistic Regression'''\n",
    "\n",
    "class LogisticRegression():\n",
    "    \n",
    "    def __init__(self, size):\n",
    "        self.w = np.random.randn(size)\n",
    "        self.b = np.random.randn(1)\n",
    "    \n",
    "    def fit(self, X_trn, y_trn,\n",
    "            X_val, y_val,\n",
    "            lr = 1e-1, epochs = 1e3,\n",
    "            show_curve = False):\n",
    "        \n",
    "        epochs = int(epochs)\n",
    "        N, D = X_trn.shape\n",
    "        \n",
    "        J_trn = np.zeros(epochs) # train loss\n",
    "        J_val = np.zeros(epochs) # validation loss\n",
    "        \n",
    "        for epoch in range(epochs): # start to train\n",
    "            # get probability\n",
    "            p_hat = self.__forward(X_trn)\n",
    "            # record training process\n",
    "            J_trn[epoch] = cross_entropy(y_trn, p_hat)\n",
    "            J_val[epoch] = cross_entropy(y_val, self.__forward(X_val))\n",
    "            # weights update    \n",
    "            self.w -= lr*(1/N)*X_trn.T@(p_hat - y_trn)\n",
    "            self.b -= lr*(1/N)*np.sum(p_hat - y_trn)\n",
    "            # print progress\n",
    "            if epoch % 250 == 0:\n",
    "                print('Epoch: {}, train error: {:.4f}, valid error: {:.4f}'.\\\n",
    "                      format(epoch, J_trn[epoch], J_val[epoch]))\n",
    "        # plot curve\n",
    "        if show_curve:\n",
    "            plt.figure(figsize = (15, 6))\n",
    "            # train plot\n",
    "            plt.subplot(121); plt.plot(J_trn)\n",
    "            plt.xlabel('epochs'); plt.ylabel('$\\mathcal{J}$')\n",
    "            plt.title('Training Curve', fontsize = 15)\n",
    "            # valid plot\n",
    "            plt.subplot(122); plt.plot(J_val)\n",
    "            plt.xlabel('epochs'); plt.ylabel('$\\mathcal{J}$')\n",
    "            plt.title('Validation Curve', fontsize = 15)\n",
    "        # return training process\n",
    "        return {'J_trn': J_trn, 'J_val': J_val}\n",
    "    \n",
    "    def __forward(self, X):\n",
    "        return sigmoid(X@self.w + self.b)\n",
    "        \n",
    "    def predict(self, X, thresh = 0.5):\n",
    "        return (self.__forward(X) >= thresh).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Assistant Functions '''\n",
    "\n",
    "def sigmoid(h, epsilon = 1e-5):\n",
    "    return 1/(1 + np.exp(-h + epsilon))\n",
    "\n",
    "def cross_entropy(y, p_hat, epsilon = 1e-3):\n",
    "    return -(1/len(y)) * np.sum(y * np.log(p_hat + epsilon)\\\n",
    "                                + (1- y) * np.log(1 - p_hat + epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedLogisticRegression(LogisticRegression):\n",
    "    '''Weighted Logistic Regression'''\n",
    "    # Assume positive: minority dataset\n",
    "    #        negative: majority dataset\n",
    "    # Weight more on positive dataset\n",
    "    # eta: control positive cases learning weights\n",
    "    #      1 is the default, < 1 reduce the weights, > 1 increase the weights\n",
    "    \n",
    "    def fit(self, X_trn, y_trn,\n",
    "            X_val, y_val,\n",
    "            eta = 1,                 # weight for positive dataset\n",
    "            lr = 1e-1, epochs = 1e3,\n",
    "            show_curve = False, verbose = 0):\n",
    "        epochs = int(epochs)\n",
    "        N, D = X_trn.shape\n",
    "        \n",
    "        J_trn = np.zeros(epochs) # train loss\n",
    "        J_val = np.zeros(epochs) # validation loss\n",
    "\n",
    "        for epoch in range(epochs): # start to train\n",
    "            # get probability for cross entropy\n",
    "            p_hat = self.__forward(X_trn)\n",
    "            # record training process\n",
    "            J_trn[epoch] = weighted_cross_entropy(y_trn, p_hat, eta = eta)\n",
    "            J_val[epoch] = weighted_cross_entropy(y_val, self.__forward(X_val), eta = eta)\n",
    "            # weights update\n",
    "            self.w -= lr*(eta*(1/len(y_trn[y_trn == 1]))*X_trn[y_trn == 1].T@(p_hat[y_trn == 1] - y_trn[y_trn == 1])+\\\n",
    "                          (1/len(y_trn[y_trn == 0]))*X_trn[y_trn == 0].T@(p_hat[y_trn == 0] - y_trn[y_trn == 0]))\n",
    "            \n",
    "            self.b -= lr*(eta*(1/len(y_trn[y_trn == 1]))*np.sum(p_hat[y_trn == 1] - y_trn[y_trn == 1])+\\\n",
    "                          (1/len(y_trn[y_trn == 0]))*np.sum(p_hat[y_trn == 0] - y_trn[y_trn == 0]))\n",
    "            # print progress\n",
    "            if verbose == 1:\n",
    "                if epoch % 250 == 0:\n",
    "                    print('Epoch: {}, train error: {:.4f}, validation error: {:.4f}'.\\\n",
    "                          format(epoch, J_trn[epoch], J_val[epoch]))\n",
    "            else:\n",
    "                pass\n",
    "        # plot curve\n",
    "        if show_curve:\n",
    "            plt.figure(figsize = (15, 6))\n",
    "            # train plot\n",
    "            plt.subplot(121); plt.plot(J_trn)\n",
    "            plt.xlabel('epochs'); plt.ylabel('$\\mathcal{J}$')\n",
    "            plt.title('Training Curve', fontsize = 15)\n",
    "            # valid plot\n",
    "            plt.subplot(122); plt.plot(J_val)\n",
    "            plt.xlabel('epochs'); plt.ylabel('$\\mathcal{J}$')\n",
    "            plt.title('Validation Curve', fontsize = 15)\n",
    "        # return training process\n",
    "        return {'J_trn': J_trn, 'J_val': J_val}\n",
    "    \n",
    "    def __forward(self, X):\n",
    "        return sigmoid(X@self.w + self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Assistant Functions '''\n",
    "\n",
    "def weighted_cross_entropy(y, p_hat, eta = 1, epsilon = 1e-3):\n",
    "    pos_w = (1/len(y[y == 1]))*eta\n",
    "    neg_w = 1/len(y[y == 0])\n",
    "    pos = -pos_w*np.sum(y[y == 1] * np.log(p_hat[y == 1] + epsilon)\\\n",
    "                        +(1- y[y == 1]) * np.log(1 - p_hat[y == 1] + epsilon))\n",
    "    neg = -neg_w*np.sum(y[y == 0] * np.log(p_hat[y == 0] + epsilon)\\\n",
    "                        +(1- y[y == 0]) * np.log(1 - p_hat[y == 0] + epsilon))\n",
    "    return pos + neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
